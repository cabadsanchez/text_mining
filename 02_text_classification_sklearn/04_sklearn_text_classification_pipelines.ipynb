{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('Sklearn version:', sklearn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=5000, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "       ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "# Fit all the pipeline\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7989347536617842\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                    remove=('headers', 'footers', 'quotes'),\n",
    "                    categories=categories, \n",
    "                    shuffle=True, random_state=42)\n",
    "\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change classifier in the pipeline\n",
    "    - LinearSVC\n",
    "    - k-NN\n",
    "    - Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8089214380825566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC())\n",
    "                    ])\n",
    "\n",
    "#Fit\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Predict\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.27363515312916115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier())\n",
    "                    ])\n",
    "\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6904127829560586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use features from a factorization instead the provided by the tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing, tokenizing and filtering of stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=5000,\n",
    "                                stop_words='english')\n",
    "X_train_counts = tf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_components = 6\n",
    "n_top_words = 20\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(X_train_counts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "church pope catholic marriage authority married orthodox canon schism mass liturgy bishop ceremony st churches catholics does priest jurisdiction coptic\n",
      "Topic #1:\n",
      "image file jpeg use program files images gif color know format does thanks graphics software using version bit available like\n",
      "Topic #2:\n",
      "edu com graphics mail send pub keyboard ftp data computer information cs systems software ca faq available gov contact pc\n",
      "Topic #3:\n",
      "god people think don jesus just does believe know say like time bible way things good true life christian question\n",
      "Topic #4:\n",
      "health use medical years people disease food msg new patients like don doctor research time 1993 10 day know just\n",
      "Topic #5:\n",
      "banks gordon skepticism edu soon pitt geb intellect chastity n3jxp dsl shameful cadre surrender father spirit son holy int col\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7003994673768309\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_lda_knn = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')),\n",
    "                         ('lda', LatentDirichletAllocation(n_components=150,\n",
    "                                                           max_iter=15,\n",
    "                                                           learning_method='online',\n",
    "                                                           learning_offset=200.,\n",
    "                                                           random_state=0)),\n",
    "                         ('clf', KNeighborsClassifier(n_neighbors=10))\n",
    "                        ])\n",
    "                         \n",
    "_ = text_lda_knn.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_lda_knn.predict(twenty_test.data)\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6731025299600533\n",
      "Wall time: 56.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_lda_rf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')),\n",
    "                         ('lda', LatentDirichletAllocation(n_components=150,\n",
    "                                                           max_iter=15,\n",
    "                                                           learning_method='online',\n",
    "                                                           learning_offset=200.,\n",
    "                                                           random_state=0)),\n",
    "                         ('clf', RandomForestClassifier()),\n",
    "                        ])\n",
    "                         \n",
    "_ = text_lda_rf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_lda_rf.predict(twenty_test.data)\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define estimator. No parameters of the search\n",
    "clf = Pipeline([('vect', CountVectorizer(max_df=.95, min_df=2)),\n",
    "                ('tfidf', TfidfTransformer()), # tf-idf\n",
    "                ('clf', LinearSVC()), # LinearSVC\n",
    "                ])\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_dist = {\"vect__max_features\": [1000, 2500, 5000, 7500, 10000, None], \n",
    "              \"vect__stop_words\": ['english', None], \n",
    "              \"clf__C\": [.1, .5, 1., 1.5, 2.]}\n",
    "\n",
    "# Define randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, return_train_score=True)\n",
    "\n",
    "# Run the randomized search\n",
    "random_search.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__stop_words</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.345919</td>\n",
       "      <td>0.115975</td>\n",
       "      <td>0.787328</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__stop_words': None, 'vect__max_features...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.802125</td>\n",
       "      <td>0.975399</td>\n",
       "      <td>0.791501</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.768309</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.010835</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.001368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.307170</td>\n",
       "      <td>0.111290</td>\n",
       "      <td>0.868409</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10000</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vect__stop_words': 'english', 'vect__max_fea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883134</td>\n",
       "      <td>0.983378</td>\n",
       "      <td>0.855246</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.866844</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.011444</td>\n",
       "      <td>0.001733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344260</td>\n",
       "      <td>0.109288</td>\n",
       "      <td>0.797519</td>\n",
       "      <td>0.961011</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vect__stop_words': 'english', 'vect__max_fea...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.803453</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.798141</td>\n",
       "      <td>0.966755</td>\n",
       "      <td>0.790945</td>\n",
       "      <td>0.958831</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>0.004101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.393379</td>\n",
       "      <td>0.115328</td>\n",
       "      <td>0.865751</td>\n",
       "      <td>0.982057</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7500</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vect__stop_words': 'english', 'vect__max_fea...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.881806</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.855246</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.860186</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.073303</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.364981</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.828977</td>\n",
       "      <td>0.951043</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__stop_words': None, 'vect__max_features...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.843293</td>\n",
       "      <td>0.954122</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.951463</td>\n",
       "      <td>0.826897</td>\n",
       "      <td>0.947543</td>\n",
       "      <td>0.060972</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>0.010947</td>\n",
       "      <td>0.002702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.304154</td>\n",
       "      <td>0.113310</td>\n",
       "      <td>0.781568</td>\n",
       "      <td>0.899203</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__stop_words': None, 'vect__max_features...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.895612</td>\n",
       "      <td>0.772908</td>\n",
       "      <td>0.902926</td>\n",
       "      <td>0.766977</td>\n",
       "      <td>0.899070</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.002987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.322188</td>\n",
       "      <td>0.110955</td>\n",
       "      <td>0.852902</td>\n",
       "      <td>0.964777</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7500</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vect__stop_words': 'english', 'vect__max_fea...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.861886</td>\n",
       "      <td>0.964761</td>\n",
       "      <td>0.851262</td>\n",
       "      <td>0.965426</td>\n",
       "      <td>0.845539</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.045290</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.000524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.408427</td>\n",
       "      <td>0.118642</td>\n",
       "      <td>0.858219</td>\n",
       "      <td>0.981170</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7500</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__stop_words': None, 'vect__max_features...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.867198</td>\n",
       "      <td>0.980053</td>\n",
       "      <td>0.848606</td>\n",
       "      <td>0.983378</td>\n",
       "      <td>0.858855</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.080976</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>0.001561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.293113</td>\n",
       "      <td>0.105941</td>\n",
       "      <td>0.782898</td>\n",
       "      <td>0.974302</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vect__stop_words': 'english', 'vect__max_fea...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.796813</td>\n",
       "      <td>0.972739</td>\n",
       "      <td>0.786189</td>\n",
       "      <td>0.975399</td>\n",
       "      <td>0.765646</td>\n",
       "      <td>0.974768</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.012932</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.111954</td>\n",
       "      <td>0.821444</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__stop_words': None, 'vect__max_features...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.827357</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.819389</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.817577</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.345919         0.115975         0.787328          0.977182   \n",
       "1       0.307170         0.111290         0.868409          0.982500   \n",
       "2       0.344260         0.109288         0.797519          0.961011   \n",
       "3       0.393379         0.115328         0.865751          0.982057   \n",
       "4       0.364981         0.128676         0.828977          0.951043   \n",
       "5       0.304154         0.113310         0.781568          0.899203   \n",
       "6       0.322188         0.110955         0.852902          0.964777   \n",
       "7       0.408427         0.118642         0.858219          0.981170   \n",
       "8       0.293113         0.105941         0.782898          0.974302   \n",
       "9       0.325203         0.111954         0.821444          0.982499   \n",
       "\n",
       "  param_clf__C param_vect__max_features param_vect__stop_words  \\\n",
       "0          1.5                     1000                   None   \n",
       "1          0.5                    10000                english   \n",
       "2          0.5                     1000                english   \n",
       "3          0.5                     7500                english   \n",
       "4          0.1                     None                   None   \n",
       "5          0.1                     1000                   None   \n",
       "6          0.1                     7500                english   \n",
       "7          0.5                     7500                   None   \n",
       "8          1.5                     1000                english   \n",
       "9          1.5                     2500                   None   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'vect__stop_words': None, 'vect__max_features...                8   \n",
       "1  {'vect__stop_words': 'english', 'vect__max_fea...                1   \n",
       "2  {'vect__stop_words': 'english', 'vect__max_fea...                7   \n",
       "3  {'vect__stop_words': 'english', 'vect__max_fea...                2   \n",
       "4  {'vect__stop_words': None, 'vect__max_features...                5   \n",
       "5  {'vect__stop_words': None, 'vect__max_features...               10   \n",
       "6  {'vect__stop_words': 'english', 'vect__max_fea...                4   \n",
       "7  {'vect__stop_words': None, 'vect__max_features...                3   \n",
       "8  {'vect__stop_words': 'english', 'vect__max_fea...                9   \n",
       "9  {'vect__stop_words': None, 'vect__max_features...                6   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.802125            0.975399           0.791501   \n",
       "1           0.883134            0.983378           0.855246   \n",
       "2           0.803453            0.957447           0.798141   \n",
       "3           0.881806            0.982048           0.855246   \n",
       "4           0.843293            0.954122           0.816733   \n",
       "5           0.804781            0.895612           0.772908   \n",
       "6           0.861886            0.964761           0.851262   \n",
       "7           0.867198            0.980053           0.848606   \n",
       "8           0.796813            0.972739           0.786189   \n",
       "9           0.827357            0.982048           0.819389   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.978723           0.768309            0.977424      0.010835   \n",
       "1            0.984043           0.866844            0.980080      0.011461   \n",
       "2            0.966755           0.790945            0.958831      0.044103   \n",
       "3            0.984043           0.860186            0.980080      0.073303   \n",
       "4            0.951463           0.826897            0.947543      0.060972   \n",
       "5            0.902926           0.766977            0.899070      0.006826   \n",
       "6            0.965426           0.845539            0.964143      0.045290   \n",
       "7            0.983378           0.858855            0.980080      0.080976   \n",
       "8            0.975399           0.765646            0.974768      0.002510   \n",
       "9            0.984043           0.817577            0.981408      0.002478   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.009997        0.014114         0.001368  \n",
       "1        0.003739        0.011444         0.001733  \n",
       "2        0.011611        0.005124         0.004101  \n",
       "3        0.003560        0.011538         0.001618  \n",
       "4        0.015227        0.010947         0.002702  \n",
       "5        0.006714        0.016602         0.002987  \n",
       "6        0.008211        0.006772         0.000524  \n",
       "7        0.005324        0.007607         0.001561  \n",
       "8        0.006950        0.012932         0.001135  \n",
       "9        0.006239        0.004249         0.001122  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dictionary of search results to a Pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_cv_results = pd.DataFrame.from_dict(random_search.cv_results_)\n",
    "df_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'vect__stop_words': 'english', 'vect__max_features': 10000, 'clf__C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print('Best params:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8202396804260985\n"
     ]
    }
   ],
   "source": [
    "# Score & evaluate test data using the best estimator\n",
    "\n",
    "predicted = random_search.predict(twenty_test.data)\n",
    "\n",
    "print('Test accuracy:', np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional metrics for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.78      0.61      0.68       319\n",
      "         comp.graphics       0.82      0.93      0.87       389\n",
      "               sci.med       0.87      0.86      0.87       396\n",
      "soc.religion.christian       0.80      0.84      0.82       398\n",
      "\n",
      "           avg / total       0.82      0.82      0.82      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(twenty_test.target, \n",
    "                                    predicted,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[193,  21,  29,  76],\n",
       "       [ 13, 361,  14,   1],\n",
       "       [ 12,  33, 342,   9],\n",
       "       [ 31,  23,   8, 336]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
